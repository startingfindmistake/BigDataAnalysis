{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c227edd4-d2d8-4ea9-8fbb-2213fc68028c",
   "metadata": {},
   "source": [
    "# 머신러닝 프로세스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc9f9d-d029-4ff2-85f7-152ee52624e4",
   "metadata": {},
   "source": [
    "## Performance_evaluation_method(성능평가방법)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea6ca6-c42a-4d1c-84f9-985a7828cd63",
   "metadata": {},
   "source": [
    "### 회귀분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f23af-749c-4e3e-b10d-0ac01f782589",
   "metadata": {},
   "source": [
    "#### (1)MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fae2cf8-b256-4ec2-823a-a9eb380c4ce7",
   "metadata": {},
   "source": [
    "* MAE (Mean Absolute Error): 예측 값(y_pred)와 실제 값(y_test) 사이의 차이(오차)의\n",
    "  절대 값을 평균한 값으로, 모델의 예측 정확도를 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f4c54-19d4-4fa9-8446-db0c61d6a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 실제 값 (y_test)와 예측 값(y_pred) 사이의 평균 절대 오차(MAE)를 계산\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# 간단히 말해 MAE는 평균 절대 오차로, 예측 값과 실제 값 사이의 오차 크기의 평균을 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbbd2be-f8a9-4f02-84b1-f9ce6c46f7c4",
   "metadata": {},
   "source": [
    "#### (2)MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf198dd-233a-4ae6-8e2d-126efaeddd73",
   "metadata": {},
   "source": [
    "* MSE (Mean Squared Error): 예측값 (y_pred)와 실제 값(y_test) 사이의 차이를 제곱한 후 평균을 낸 값으로\n",
    "  모델의 예측 오차를 평가하는 지표 입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beacd67-9e2f-4232-8f2b-d6d843856fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 실제 값(y_test)와 예측 값(y_pred) 사이의 평균 제곱 오차(MSE)를 계산\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# 간단히 말해 MSE는 예측 값과 실제 값 사이의 차이(오차)의 제곱을 평균한 값으로, 모델의 예측 오차를 측정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78212375-e3f4-47b8-bea3-0b1125fb1066",
   "metadata": {},
   "source": [
    "#### (3)RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d36d1d-6e69-4168-97ce-f83330be659e",
   "metadata": {},
   "source": [
    "* MSE: 예측 값과 실제 값 간의 차이를 제곱한 후 평균을 낸 값\n",
    "* RMSE (Root Mean Squared Error): MSE의 제곱근을 구하여 오차의 크기를 더 직관적으로 이해할 수 있도록\n",
    "  변환한 값, 모델의 예측 오차를 평가하는 데 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af3919-93e4-46d0-a71b-fcc4754fd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 실제 값(y_test)와 예측 값(y_pred) 사이의 평균 제곱 오차(MSE)를 계산\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# MSE의 제곱근을 구하여 제곱근 평균 제곱 오차(RMSE)를 계산\n",
    "rmse = np.sqrt(mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7681999-1dba-45d6-a947-939423a257f1",
   "metadata": {},
   "source": [
    "#### (4)MSLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a047e5e0-d881-4604-910c-dd8ccfdb6e59",
   "metadata": {},
   "source": [
    "* MLSE (Mean Squared Logarithmic Error): 로그 변환된 예측 값과 실제 값 간의 차이를 제곱한 후\n",
    "  평균을 구한 값입니다. 주로 값이 음수가 될 수 없거나 큰 값의 차이를 줄이는 데 유용한 평가 지표입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7e2fc-714f-443c-a58c-03a55d7d44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "\n",
    "# 실제 값(y_test)와 예측 값(y_pred) 사이의 평균 제곱 로그 오차(MLSE)를 계산\n",
    "msle = mean_squared_log_error(y_test, y_pred)\n",
    "\n",
    "# mlse는 로그 변환된 예측 값과 실제 값 사이의 제곱 차이를 측정한 값입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b9a16-d6fd-41f6-a34e-c4bd268c06c2",
   "metadata": {},
   "source": [
    "#### (5)MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c56724-4f5a-423c-a240-c7a6e37ae2ca",
   "metadata": {},
   "source": [
    "* MAPE (Mean Absolute Percentage Error): 예측 값과 실제 값 간의 차이(오차)를 실제 값에 대한\n",
    "  백분율로 계산한 후, 그 평균을 구하는 지표입니다. 결과는 백분율로 출력되며, 예측 오류를 비율로 쉽게\n",
    "  해석할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4f6c2-9f51-45ad-a631-7a8729b5bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # 숫자 연산을 위한 numpy 불러오기\n",
    "\n",
    "# 평균 절대 퍼센트 오차 (MAPE)를 계산하는 함수 정의\n",
    "def MAPE(y_test, y_pred):\n",
    "    # 절대 퍼센트 오차를 계산하고 그 평균을 100으로 곱해 반환\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    return mape\n",
    "\n",
    "# 실제 값(y_test)와 예측 값(y_pred)을 사용해 MAPE 함수를 호출\n",
    "mape = MAPE(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0eac30-fa9a-4e6a-9894-621fcf0872dd",
   "metadata": {},
   "source": [
    "## 분류분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbda219-9c1b-48c8-930a-c05cd3383d1c",
   "metadata": {},
   "source": [
    "#### (1)정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e6f61-3ca3-43ef-9a69-4bdae2c49a5b",
   "metadata": {},
   "source": [
    "* Accuracy Score(정확도): 예측 값이 실제 값과 일치한 비율을 계산하는 지표입니다.\n",
    "  주로 분류 문제에서 사용되며, 정확도가 높을수록 모델의 예측이 더 정확하다는는 것을 의미합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ac898-eb5c-4de0-9d0a-25f3f1e097a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 실제 라벨(y_test)와 예측 라벨(y_pred) 사이의 정확도를 계산\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# acc는 정확도를 나타내며, 이는 전체 예측 중에서 맞춘 비율을 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a2eb7a-b6df-4543-a9ed-f49f5afacf40",
   "metadata": {},
   "source": [
    "#### (2)혼동행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23424ac3-6757-4de4-afe7-97856cbddd63",
   "metadata": {},
   "source": [
    "* Confsion Matrix(혼동 행렬): 분류 모델의 성능을 평가하는 지표로, 예측 결과와 실제 값을 바탕으로 네 가지 경우\n",
    "  (참 긍정,참 부정, 거짓 긍정, 거짓 부정)을 요약하여 보여준다.\n",
    "  이를 통해 모델의 분류 오류를 직관적으로 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc86526-da9c-4750-929e-16b929bb181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 실제 라벨(y_test)와 예측 라벨(y_pred) 사이의 혼동 행렬을 계산\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# cm은 혼동 행렬로, 분류 모델의 성능을 요약하여 보여줌\n",
    "\n",
    "# 행렬은 True Positive, True Negative, False Positive, Fasle Negative를 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f688f1-9a4a-4595-bc2e-90f9ea63726c",
   "metadata": {},
   "source": [
    "#### (3)정밀도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0716c471-0a27-452f-8b7a-a160abb24e99",
   "metadata": {},
   "source": [
    "* Precision(정밀도): 모델이 양성으로 예측한 값 중 실제로 양성인 값의 비율을 나타냅니다.\n",
    "  높은 정밀도는 False Positive(거짓 양성)의 비율이 낮다는 것을 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd80e95-8f5e-4036-8a15-eea9e56cc38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# 실제 라벨(y_test)와 예측 라벨(y_pred) 사이의 정밀도를 게산\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "#precision은 정밀도를 나타내며, 이는 True Positive와 False Positive의 합에서 True Positive의 비율을 의미함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41da848-ae52-427c-b33e-57d5a24d74b9",
   "metadata": {},
   "source": [
    "* Recall(재현율): 실제 양성 중에서 모델이 양성으로 정확히 예측한 비율입니다.\n",
    "  높은 재현율은 False Negative(거짓 음성)의 비율이 낮음을 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f51c54-b32b-4bae-9342-9375cd2b9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# 실제 라벨(y_test)와 예측 라벨(y_pred) 사이의 재현율을 계산\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# recall은 재현율을 나타내며, 이는 True Positive와 False Negative의 합에서 True Positive의 비율을 의미함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d82e6-5998-426c-9b7a-8a035a4a882c",
   "metadata": {},
   "source": [
    "#### (4) F1 스코어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f25f5-6fe9-4106-a22c-bab85e0b4385",
   "metadata": {},
   "source": [
    "* F1 Score: 정밀도(Precision)와 재현율(Recall)의 조화 평균을 나타내며, 두 지표의 균형을 평가하는 데\n",
    "  사용됩니다. F1점수는 특히 부균형한 클래스 문제에서 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6342b2-d990-488a-ae4b-265459b21734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 실제 라벨(y_test)와 예측 라벨(y_pred) 사이의 F1 점수를 계산\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# f1은 F1 점수를 나타내며, 이는 정밀도와 재현율의 조화 평균이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fed1c5-8fba-4549-8052-b0f4734fe1bf",
   "metadata": {},
   "source": [
    "#### (5) ROC곡선과 AUC스코어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa01bb-fe2c-4605-a0da-196b3f7b8890",
   "metadata": {},
   "source": [
    "* ROC Curve (Receiver Operating Characteristic Curve): 위양성률(FPR)과 재현율(TPR)의 관계를\n",
    "  나타내는 그래프입니다. 이는 분류 모델의 성능을 다양한 임계값에서 평가할 수 있게 해주며, 완벽한 모델일수록\n",
    "  ROC 곡선이 좌상단에 가까워집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afab40-5492-443e-8bea-62076e8fe70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# ROC 곡선을 그리기 위해 FPR(위양성률), TPR(재현율), 임계값을 계산\n",
    "fqr, tpr, thres = roc_curve(y_test, y_pred, pos_label=1)\n",
    "\n",
    "import matplotlib.pyplot as plt # 그래프 그리기를 위한 matplotlib 불러오기\n",
    "\n",
    "plt.plot(fqr, tpr)  # FPR와 TPR 값을 사용하여 ROC 곡선을 그림\n",
    "\n",
    "# 축에 레이블 추갛여 명확하게 표시\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608fc934-8ebe-49df-802b-0284d387cecc",
   "metadata": {},
   "source": [
    "* AUC (Area Under the Curve): ROC 곡선 아래의 면적을 나타내며, 분류 모델의 성능을 평가하는 데 사용됩니다.\n",
    "  AUC 값이 1에 가까울수록 모델의 성능이 뛰어납니다.\n",
    "  AUC는 모델이 임의로 양성/음성 클래스를 구분하는 능력을 평가하는 지표입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6fac8-b1c8-46b8-bca8-1a9c592ebdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ROC 곡선을 그리기 위해 FPR(위양성률), TPR(재현율), 임계값을 계산\n",
    "fpr, tpr, thres = roc_curve(y_test, y_pred, pos_label=1)\n",
    "\n",
    "# FPR와 TPR을 사용하여 AUC(곡선 아래 면적) 값을 계산\n",
    "auc_value = auc(fpr, tpr)\n",
    "\n",
    "print('AUC Score:', auc_value)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
